{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-layered perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoJevCnWB398",
        "outputId": "9647fa70-5b0b-4a4e-e7a8-20a70fd4d2c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59DyOhnE8iKQ"
      },
      "source": [
        "dataset.csv created by the dataset_creation.py scipt is extremely skewed with ratio of 1:11. So undersampled the majority label so that data has both the labels in 1:1 ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeaeINuka-u_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"drive/MyDrive/NLP-Project/dataset.csv\")\n",
        "df.drop('Features', inplace=True, axis=1)\n",
        "df = df.sort_values(by=[\"Label\"], ascending=True)\n",
        "df = df[0:50000]\n",
        "bodyid = [i for i in range(50000)]\n",
        "df.insert(0, \"Body ID\", bodyid, True)\n",
        "df.groupby(\"Label\").count()\n",
        "df.columns = ['Body ID', 'Headline', 'articleBody', 'Stance']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EJ_0Kb9IgI2N",
        "outputId": "38ac6617-509a-4478-8180-91976f79aabb"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Headline</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>food snack selection popular greek dishes</td>\n",
              "      <td>underwhelmed main courses tables sometimes har...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20343</th>\n",
              "      <td>1</td>\n",
              "      <td>deal walkin wait nearly min guarantee well wor...</td>\n",
              "      <td>go long broth bit short noodles meat order dis...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20342</th>\n",
              "      <td>2</td>\n",
              "      <td>best ramen new york city</td>\n",
              "      <td>sat bar twice always interesting watching huge...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20341</th>\n",
              "      <td>3</td>\n",
              "      <td>obsessed ramen love totto ramen</td>\n",
              "      <td>miso ramen way salty sort heavy noodles great ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20340</th>\n",
              "      <td>4</td>\n",
              "      <td>best ramen world</td>\n",
              "      <td>loooooooooong wait prepare go lunch hour dinner</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Body ID  ... Stance\n",
              "0            0  ...      0\n",
              "20343        1  ...      0\n",
              "20342        2  ...      0\n",
              "20341        3  ...      0\n",
              "20340        4  ...      0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsPSOz4A-ONF"
      },
      "source": [
        "Split to train and test data in the ratio 8:2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfNY5XuBcoNS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHn_V8T4-77g"
      },
      "source": [
        "Both test and train has balanced labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "2xEiqYVFeAdN",
        "outputId": "3d0a7a96-92b7-41be-ed7e-0821c1c0cc98"
      },
      "source": [
        "train.groupby(\"Stance\").count()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Headline</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Stance</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19920</td>\n",
              "      <td>19920</td>\n",
              "      <td>19920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20080</td>\n",
              "      <td>20080</td>\n",
              "      <td>20080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Body ID  Headline  articleBody\n",
              "Stance                                \n",
              "0         19920     19920        19920\n",
              "1         20080     20080        20080"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "7uS3Iq9reIK1",
        "outputId": "21188e0e-8f20-436d-f34f-df4e45bcc316"
      },
      "source": [
        "test.groupby(\"Stance\").count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Headline</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Stance</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4983</td>\n",
              "      <td>4983</td>\n",
              "      <td>4983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5017</td>\n",
              "      <td>5017</td>\n",
              "      <td>5017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Body ID  Headline  articleBody\n",
              "Stance                                \n",
              "0          4983      4983         4983\n",
              "1          5017      5017         5017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYDsAx2Q_B31"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QebDof1ceKu-"
      },
      "source": [
        "train.to_csv(\"drive/MyDrive/NLP-Project/train.csv\")\n",
        "test.to_csv(\"drive/MyDrive/NLP-Project/test.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aZOZXtnQsOKS",
        "outputId": "bb18c41d-cf7d-4e11-9017-d65d756af67b"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Headline</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>198852</th>\n",
              "      <td>28384</td>\n",
              "      <td>cafe run cleaver company awesome catering comp...</td>\n",
              "      <td>table serves organic wines happy hour friday e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66816</th>\n",
              "      <td>12300</td>\n",
              "      <td>excellent addition new york</td>\n",
              "      <td>food average gastro style pub food devilled eg...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157373</th>\n",
              "      <td>15539</td>\n",
              "      <td>enjoy pizza man delicious</td>\n",
              "      <td>dollars regular slice pizza yes agree price ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204417</th>\n",
              "      <td>11521</td>\n",
              "      <td>absolutely best food new york</td>\n",
              "      <td>highlight trip could eat days thanks camile ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40244</th>\n",
              "      <td>6174</td>\n",
              "      <td>review removed piora yelp reviews</td>\n",
              "      <td>list bucatini salty octopus salty wine way pri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Body ID  ... Stance\n",
              "198852    28384  ...      1\n",
              "66816     12300  ...      0\n",
              "157373    15539  ...      0\n",
              "204417    11521  ...      0\n",
              "40244      6174  ...      0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEgx-8VS_F2r"
      },
      "source": [
        "Ready the data for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfo9WxkWoETq"
      },
      "source": [
        "train_bodies = train.reindex(columns=['Body ID', 'articleBody'])\n",
        "train_stances = train.reindex(columns=['Headline', 'Body ID', 'Stance'])\n",
        "test_bodies = test.reindex(columns=['Body ID', 'articleBody'])\n",
        "test_stances_unlabeled = test.reindex(columns=['Headline', 'Body ID'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7S4nbf0sMNh"
      },
      "source": [
        "train_stances.to_csv(\"drive/MyDrive/NLP-Project/train_stances.csv\")\n",
        "train_bodies.to_csv(\"drive/MyDrive/NLP-Project/train_bodies.csv\")\n",
        "test_stances_unlabeled.to_csv(\"drive/MyDrive/NLP-Project/test_stances_unlabeled.csv\")\n",
        "test_bodies.to_csv(\"drive/MyDrive/NLP-Project/test_bodies.csv\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q84nkYfZF7vA",
        "outputId": "fafa7452-17e8-44a8-a5f2-807a56161f13"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dWLkRoOGA_a",
        "outputId": "1569533a-a9f1-4e2f-ff67-bef741ad621b"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfO3S_SY_cZn"
      },
      "source": [
        "Import relevant packages and modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8KHEzx__gNR"
      },
      "source": [
        "from csv import DictReader\n",
        "from csv import DictWriter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tkL9uoO_onW"
      },
      "source": [
        "Initialise global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhND63-__sly"
      },
      "source": [
        "label_ref = {'0': 0, '1': 1}\n",
        "label_ref_rev = {0: '0', 1: '1'}\n",
        "stop_words = [\n",
        "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
        "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
        "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
        "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
        "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
        "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
        "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
        "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
        "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
        "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
        "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
        "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
        "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
        "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
        "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
        "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
        "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
        "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
        "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
        "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
        "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
        "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
        "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
        "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
        "        ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxiJmQdUAHbp"
      },
      "source": [
        "Define all the utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv69n6B7CpHJ"
      },
      "source": [
        "\n",
        "# Define data class\n",
        "class FNCData:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Define class for Yelp fake review data\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_instances, file_bodies):\n",
        "\n",
        "        # Load data\n",
        "        self.instances = self.read(file_instances)\n",
        "        bodies = self.read(file_bodies)\n",
        "        self.heads = {}\n",
        "        self.bodies = {}\n",
        "\n",
        "        # Process instances\n",
        "        for instance in self.instances:\n",
        "            if instance['Headline'] not in self.heads:\n",
        "                head_id = len(self.heads)\n",
        "                self.heads[instance['Headline']] = head_id\n",
        "            instance['Body ID'] = int(instance['Body ID'])\n",
        "\n",
        "        # Process bodies\n",
        "        for body in bodies:\n",
        "            self.bodies[int(body['Body ID'])] = body['articleBody']\n",
        "\n",
        "    def read(self, filename):\n",
        "\n",
        "        \"\"\"\n",
        "        Read Fake News Challenge data from CSV file\n",
        "\n",
        "        Args:\n",
        "            filename: str, filename + extension\n",
        "\n",
        "        Returns:\n",
        "            rows: list, of dict per instance\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialise\n",
        "        rows = []\n",
        "\n",
        "        # Process file\n",
        "        with open(filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "\n",
        "        return rows\n",
        "\n",
        "\n",
        "# Define relevant functions\n",
        "def pipeline_train(train, test, lim_unigram):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Process train set, create relevant vectorizers\n",
        "\n",
        "    Args:\n",
        "        train: FNCData object, train set\n",
        "        test: FNCData object, test set\n",
        "        lim_unigram: int, number of most frequent words to consider\n",
        "\n",
        "    Returns:\n",
        "        train_set: list, of numpy arrays\n",
        "        train_stances: list, of ints\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise\n",
        "    heads = []\n",
        "    heads_track = {}\n",
        "    bodies = []\n",
        "    bodies_track = {}\n",
        "    body_ids = []\n",
        "    id_ref = {}\n",
        "    train_set = []\n",
        "    train_stances = []\n",
        "    cos_track = {}\n",
        "    test_heads = []\n",
        "    test_heads_track = {}\n",
        "    test_bodies = []\n",
        "    test_bodies_track = {}\n",
        "    test_body_ids = []\n",
        "    head_tfidf_track = {}\n",
        "    body_tfidf_track = {}\n",
        "\n",
        "    # Identify unique heads and bodies\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            heads.append(head)\n",
        "            heads_track[head] = 1\n",
        "        if body_id not in bodies_track:\n",
        "            bodies.append(train.bodies[body_id])\n",
        "            bodies_track[body_id] = 1\n",
        "            body_ids.append(body_id)\n",
        "\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in test_heads_track:\n",
        "            test_heads.append(head)\n",
        "            test_heads_track[head] = 1\n",
        "        if body_id not in test_bodies_track:\n",
        "            test_bodies.append(test.bodies[body_id])\n",
        "            test_bodies_track[body_id] = 1\n",
        "            test_body_ids.append(body_id)\n",
        "\n",
        "    # Create reference dictionary\n",
        "    for i, elem in enumerate(heads + body_ids):\n",
        "        id_ref[elem] = i\n",
        "\n",
        "    # Create vectorizers and BOW and TF arrays for train set\n",
        "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "    bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
        "\n",
        "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "        fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
        "\n",
        "    # Process train set\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
        "        body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
        "        if head not in head_tfidf_track:\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
        "            head_tfidf_track[head] = head_tfidf\n",
        "        else:\n",
        "            head_tfidf = head_tfidf_track[head]\n",
        "        if body_id not in body_tfidf_track:\n",
        "            body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
        "            body_tfidf_track[body_id] = body_tfidf\n",
        "        else:\n",
        "            body_tfidf = body_tfidf_track[body_id]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        train_set.append(feat_vec)\n",
        "        train_stances.append(label_ref[instance['Stance']])\n",
        "\n",
        "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer\n",
        "\n",
        "\n",
        "def pipeline_test(test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Process test set\n",
        "\n",
        "    Args:\n",
        "        test: FNCData object, test set\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "\n",
        "    Returns:\n",
        "        test_set: list, of numpy arrays\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise\n",
        "    test_set = []\n",
        "    heads_track = {}\n",
        "    bodies_track = {}\n",
        "    cos_track = {}\n",
        "\n",
        "    # Process test set\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            head_bow = bow_vectorizer.transform([head]).toarray()\n",
        "            head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "            heads_track[head] = (head_tf, head_tfidf)\n",
        "        else:\n",
        "            head_tf = heads_track[head][0]\n",
        "            head_tfidf = heads_track[head][1]\n",
        "        if body_id not in bodies_track:\n",
        "            body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
        "            body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
        "            body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
        "            bodies_track[body_id] = (body_tf, body_tfidf)\n",
        "        else:\n",
        "            body_tf = bodies_track[body_id][0]\n",
        "            body_tfidf = bodies_track[body_id][1]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        test_set.append(feat_vec)\n",
        "\n",
        "    return test_set\n",
        "\n",
        "\n",
        "def load_model(sess):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Load TensorFlow model\n",
        "\n",
        "    Args:\n",
        "        sess: TensorFlow session\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, './model/model.checkpoint')\n",
        "\n",
        "\n",
        "def save_predictions(pred, file):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Save predictions to CSV file\n",
        "\n",
        "    Args:\n",
        "        pred: numpy array, of numeric predictions\n",
        "        file: str, filename + extension\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file, 'w') as csvfile:\n",
        "        fieldnames = ['Stance']\n",
        "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for instance in pred:\n",
        "            writer.writerow({'Stance': label_ref_rev[instance]})"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IVMlu_jBaPF"
      },
      "source": [
        "Train the model and then test it, store the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeaSvBBkCwIB",
        "outputId": "cf9b3084-476d-4b22-98cb-6beefd4a1fb2"
      },
      "source": [
        "mode = 'train'\n",
        "\n",
        "# Set file names\n",
        "file_train_instances = \"drive/MyDrive/NLP-Project/train_stances.csv\"\n",
        "file_train_bodies = \"drive/MyDrive/NLP-Project/train_bodies.csv\"\n",
        "file_test_instances = \"drive/MyDrive/NLP-Project/test_stances_unlabeled.csv\"\n",
        "file_test_bodies = \"drive/MyDrive/NLP-Project/test_bodies.csv\"\n",
        "file_predictions = 'drive/MyDrive/NLP-Project/predictions_test.csv'\n",
        "\n",
        "\n",
        "# Initialise hyperparameters\n",
        "r = random.Random()\n",
        "lim_unigram = 3000 ## 5000\n",
        "target_size = 4\n",
        "hidden_size = 100\n",
        "train_keep_prob = 0.6\n",
        "l2_alpha = 0.00001\n",
        "learn_rate = 0.01\n",
        "clip_ratio = 5\n",
        "batch_size_train = 500\n",
        "epochs = 90\n",
        "\n",
        "\n",
        "# Load data sets\n",
        "raw_train = FNCData(file_train_instances, file_train_bodies)\n",
        "raw_test = FNCData(file_test_instances, file_test_bodies)\n",
        "n_train = len(raw_train.instances)\n",
        "\n",
        "\n",
        "# Process data sets\n",
        "train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = \\\n",
        "    pipeline_train(raw_train, raw_test, lim_unigram=lim_unigram)\n",
        "feature_size = len(train_set[0])\n",
        "test_set = pipeline_test(raw_test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)\n",
        "\n",
        "\n",
        "# Define model\n",
        "\n",
        "# Create placeholders\n",
        "features_pl = tf.placeholder(tf.float32, [None, feature_size], 'features')\n",
        "stances_pl = tf.placeholder(tf.int64, [None], 'stances')\n",
        "keep_prob_pl = tf.placeholder(tf.float32)\n",
        "\n",
        "# Infer batch size\n",
        "batch_size = tf.shape(features_pl)[0]\n",
        "\n",
        "# Define multi-layer perceptron\n",
        "hidden_layer = tf.nn.dropout(tf.nn.relu(tf.contrib.layers.linear(features_pl, hidden_size)), keep_prob=keep_prob_pl)\n",
        "logits_flat = tf.nn.dropout(tf.contrib.layers.linear(hidden_layer, target_size), keep_prob=keep_prob_pl)\n",
        "logits = tf.reshape(logits_flat, [batch_size, target_size])\n",
        "\n",
        "# Define L2 loss\n",
        "tf_vars = tf.trainable_variables()\n",
        "l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf_vars if 'bias' not in v.name]) * l2_alpha\n",
        "\n",
        "# Define overall loss\n",
        "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=stances_pl) + l2_loss)\n",
        "\n",
        "# Define prediction\n",
        "softmaxed_logits = tf.nn.softmax(logits)\n",
        "predict = tf.arg_max(softmaxed_logits, 1)\n",
        "\n",
        "\n",
        "# Define optimiser\n",
        "opt_func = tf.train.AdamOptimizer(learn_rate)\n",
        "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tf_vars), clip_ratio)\n",
        "opt_op = opt_func.apply_gradients(zip(grads, tf_vars))\n",
        "\n",
        "# Perform training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        indices = list(range(n_train))\n",
        "        r.shuffle(indices)\n",
        "\n",
        "        for i in range(n_train // batch_size_train):\n",
        "            batch_indices = indices[i * batch_size_train: (i + 1) * batch_size_train]\n",
        "            batch_features = [train_set[i] for i in batch_indices]\n",
        "            batch_stances = [train_stances[i] for i in batch_indices]\n",
        "\n",
        "            batch_feed_dict = {features_pl: batch_features, stances_pl: batch_stances, keep_prob_pl: train_keep_prob}\n",
        "            _, current_loss = sess.run([opt_op, loss], feed_dict=batch_feed_dict)\n",
        "            total_loss += current_loss\n",
        "\n",
        "\n",
        "    # Predict\n",
        "    test_feed_dict = {features_pl: test_set,  keep_prob_pl: 1.0}\n",
        "    test_pred = sess.run(predict, feed_dict=test_feed_dict)\n",
        "\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(test_pred, file_predictions)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-15-032a473faad9>:70: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuwXAgX6fPca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467c616f-85f5-4d17-9ef6-aaf90c46f0a5"
      },
      "source": [
        "import csv\n",
        "\n",
        "file_predictions = 'drive/MyDrive/NLP-Project/predictions_test.csv'\n",
        "file_test = 'drive/MyDrive/NLP-Project/test.csv'\n",
        "\n",
        "predicted_label = []\n",
        "test_label = []\n",
        "\n",
        "with open(file_predictions, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "print(len(data))\n",
        "data = data[1:]\n",
        "for i in data:\n",
        "    predicted_label.append(i[0])\n",
        "\n",
        "\n",
        "with open(file_test, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "print(len(data))\n",
        "data = data[1:]\n",
        "for i in data:\n",
        "    test_label.append(i[4])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10001\n",
            "10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZyOMtDEBoZW"
      },
      "source": [
        "Calculate the evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzvxv5NamqgZ",
        "outputId": "efbf4cd3-87e4-4d9d-ec72-e7f24eb54f57"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "print(\"f1_score(macro): {}\".format(f1_score(test_label, predicted_label, average=\"macro\")))\n",
        "print(\"accuracy_score: {}\".format(accuracy_score(test_label, predicted_label)))\n",
        "print(\"precision_score(macro): {}\".format(precision_score(test_label, predicted_label, average=\"macro\")))\n",
        "print('recall_score(macro): {}'.format(recall_score(test_label, predicted_label, average=\"macro\")))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1_score(macro): 0.7076999269249817\n",
            "accuracy_score: 0.7077\n",
            "precision_score(macro): 0.7077115012946651\n",
            "recall_score(macro): 0.7077098811262259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MNgavKC7q6N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}